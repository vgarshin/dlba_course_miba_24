{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c5d7c8-aa87-44a2-8d94-2426e8f2466a",
   "metadata": {},
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f62655-2e46-4de8-95ed-ceb1f578baba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOPIC 5: Object detection problem. YOLO training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aa981-0f41-4a8e-b523-0d3e3d4b761b",
   "metadata": {},
   "source": [
    "### 1. Libraries and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa90d67-0ed9-4eae-a585-f61fc22c09ee",
   "metadata": {},
   "source": [
    "#### 1.1. Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04217bb0-3144-443f-a9f6-c7f92a7d26b9",
   "metadata": {},
   "source": [
    "[Streamlit](https://streamlit.io/) is a framework that offers a faster way to build and share data applications. It helps you to turn data scripts into shareable web apps in minutes. It is written in pure Python and does not require front‑end experience to work with. Installation is very simple in our environment. Just use terminal or type here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bb1f7-311f-4021-8540-ac1b4da2694a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34823bc0-94e4-44b9-a0d7-96702d08e99e",
   "metadata": {},
   "source": [
    "#### 1.2. Other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390f276-4198-42cd-aa1c-d3c7a70d0f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python ultralytics transformers deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80cba5-41d0-4279-b266-1dd16b663878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de48cf-d797-4cae-819c-69a586b839ee",
   "metadata": {},
   "source": [
    "#### 1.3. Free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e25f1-3fa9-40c1-b59d-b7ac662dce5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!df -h | grep dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0334c25-dfff-460b-b521-46dd8952b3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a cache dir for Huggin Face Hub models\n",
    "!ls -ls ~/.cache/huggingface/hub\n",
    "\n",
    "# a cache dir for PyTorch models\n",
    "!ls -ls ~/.cache/torch/hub/\n",
    "\n",
    "# a cache dir for DeepFace models\n",
    "!ls -la ~/.deepface/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e8946-ab4e-411c-a0e9-bbc90d29695a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use `rm -rf` !!! WITH CARE !!!\n",
    "\n",
    "!rm -rf ~/.cache/huggingface/hub\n",
    "!rm -rf ~/.cache/torch/hub/checkpoints\n",
    "!rm -rf ~/.deepface/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d1367-0a9e-4f7c-bb8f-b798ce52d5a6",
   "metadata": {},
   "source": [
    "### 2. How Streamlit works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb7058-ad37-458b-b278-044806419a31",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Main concepts](https://docs.streamlit.io/library/get-started/main-concepts) require you to create a normal Python script with all necessary elements for your future app and run it with `streamlit run` like `streamlit run your_script.py [-- script args]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e1028-ada6-46e6-85e9-002086c6f6f7",
   "metadata": {},
   "source": [
    "#### 2.1. Python script with app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa5935-7b74-4b17-b664-7113b6d8a469",
   "metadata": {},
   "source": [
    "Streamlit's architecture allows you to write apps the same way you write plain Python scripts. Let's create the sample script with `%%writefile` magic command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3877b6c-2817-4f68-8825-419a8f8a99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "# Title of our demo app\n",
    "st.title('Meet the first Streamlit application')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74095332-886b-4cda-8169-a0642cd53aab",
   "metadata": {},
   "source": [
    "#### 2.2. Run application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d23de-7527-44bb-a4ff-a505f7fa7acb",
   "metadata": {},
   "source": [
    "Run application is very easy. Just open a terminal in the folder with your Python script `stapp.py` and type:\n",
    "\n",
    "`streamlit run stapp.py --server.port 20000 --browser.gatherUsageStats False` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951f3ce-28a5-45a9-ac31-e04fbbe002a5",
   "metadata": {},
   "source": [
    "Your Streamlit application will be available with the following URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d15b55-a38a-4888-90d8-b0b26aa99f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Streamlit available at:',\n",
    "      'https://jhas01.gsom.spbu.ru{}proxy/{}/'.format(\n",
    "          os.environ['JUPYTERHUB_SERVICE_PREFIX'], 20000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498d5c1-a3fb-4050-bc78-b102d8668511",
   "metadata": {},
   "source": [
    "#### 2.2. Basic examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c63dd-c203-4952-ad67-f29d478a9d43",
   "metadata": {},
   "source": [
    "##### 2.2.1. Nice headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd66dbb-36fa-433d-829c-9a87340dd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.header('Nice looking header string', divider='rainbow')\n",
    "st.header('_Here is header under the line_ :fire:')\n",
    "\n",
    "st.subheader('Subheader is also here', divider='rainbow')\n",
    "st.subheader(':blue[_We like Streamlit_] :star:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb65cc-d528-48fe-9905-5eab89d38ab8",
   "metadata": {},
   "source": [
    "##### 2.2.2. Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334000c-10bb-449c-a2f7-9b1bdf6dda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.header('Just a header', divider='rainbow')\n",
    "st.text('Just a text under the header')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f29cf-9af2-4687-a3fd-8639e809c3f4",
   "metadata": {},
   "source": [
    "##### 2.2.3. Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d83f88-dad4-477a-a817-23bf578f7fe3",
   "metadata": {},
   "source": [
    "Along with magic commands, `st.write()` is Streamlit's \"Swiss Army knife\". You can pass almost anything to `st.write()`: text, data, Matplotlib figures, charts and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fbd00-8d22-4b2e-9adc-e5b72090108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "st.header('Demo of write function', divider='rainbow')\n",
    "st.subheader('Table and plot at one application')\n",
    "\n",
    "st.divider()\n",
    "\n",
    "st.write(\"Here's demo table from the dataframe:\")\n",
    "fruits_data = pd.DataFrame(\n",
    "    {\n",
    "        'fruits': ['apple', 'peach', 'pineapple', 'watermelon'],\n",
    "        'color': ['green', 'orange', 'yellow', 'stripes'],\n",
    "        'weight': [1, 2, 5, 10]\n",
    "    }\n",
    ")\n",
    "st.write(fruits_data)\n",
    "\n",
    "st.divider()\n",
    "\n",
    "st.write(\"Here's demo chart for fruits:\")\n",
    "chart_data = pd.DataFrame(\n",
    "     np.random.randn(20, 4),\n",
    "     columns=['apple', 'peach', 'pineapple', 'watermelon']\n",
    ")\n",
    "st.line_chart(chart_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccde3d-f9a5-4ac6-bbdd-faa65c5d82b9",
   "metadata": {},
   "source": [
    "### 3. AI with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959f5da-09e9-4109-99fa-db3da3ac1407",
   "metadata": {},
   "source": [
    "#### 3.1. Upload pipeline for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80d0fa-f4d0-4da9-8ce2-0989032f70b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        bytes_data = uploaded_file.read()\n",
    "        img = Image.open(io.BytesIO(bytes_data))\n",
    "        st.divider()\n",
    "        st.image(img, caption='Uploaded image')\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889616e-5f2e-4ff2-8a92-c9928b93ceb0",
   "metadata": {},
   "source": [
    "#### 3.2. Add some AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2510ba-77f7-4872-96e4-51765bd3bcf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# use of AI model for image captioning\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "\n",
    "\n",
    "def img_caption(model, processor, img, text=None):\n",
    "    \"\"\"\n",
    "    Uses BLIP model to caption image.\n",
    "    \n",
    "    \"\"\"\n",
    "    res = None\n",
    "    if text:\n",
    "        # conditional image captioning\n",
    "        inputs = processor(img, text, return_tensors='pt')\n",
    "    else:\n",
    "        # unconditional image captioning\n",
    "        inputs = processor(img, return_tensors='pt')\n",
    "    out = model.generate(**inputs)\n",
    "    res = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_CAP_NAME = 'Salesforce/blip-image-captioning-base'\n",
    "    PROCESSOR_CAP = BlipProcessor.from_pretrained(MODEL_CAP_NAME)\n",
    "    MODEL_CAP = BlipForConditionalGeneration.from_pretrained(MODEL_CAP_NAME)\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image with AI caption')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        # input text for conditional image captioning\n",
    "        text = st.text_input(\n",
    "            'Input text for conditional image captioning (if needed)', \n",
    "            ''\n",
    "        )\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # use image caption model for uploaded image\n",
    "            caption = img_caption(\n",
    "                model=MODEL_CAP, \n",
    "                processor=PROCESSOR_CAP, \n",
    "                img=img, \n",
    "                text=text\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption=caption)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0a903-603a-4e72-bb05-f687aee8fef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of AI model for image captioning\n",
    "# and object detection at the image\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "def img_caption(model, processor, img, text=None):\n",
    "    \"\"\"\n",
    "    Uses BLIP model to caption image.\n",
    "    \n",
    "    \"\"\"\n",
    "    res = None\n",
    "    if text:\n",
    "        # conditional image captioning\n",
    "        inputs = processor(img, text, return_tensors='pt')\n",
    "    else:\n",
    "        # unconditional image captioning\n",
    "        inputs = processor(img, return_tensors='pt')\n",
    "    out = model.generate(**inputs)\n",
    "    res = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def img_detect(model, img, plot=False):\n",
    "    \"\"\"\n",
    "    Run YOLO inference on an image.\n",
    "    \n",
    "    \"\"\"\n",
    "    result = model(img)[0]\n",
    "    boxes = result.boxes  # boxes object for bounding box outputs\n",
    "    names = model.names\n",
    "    objs = []\n",
    "    for c, p in zip(boxes.cls, boxes.conf):\n",
    "        objs.append({names[int(c)]: p.item()})\n",
    "    img_bgr = result.plot()  # BGR-order numpy array\n",
    "    img_rgb = Image.fromarray(img_bgr[..., ::-1])  # RGB-order PIL image\n",
    "    if plot:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.show()\n",
    "    return objs, img_rgb\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_CAP_NAME = 'Salesforce/blip-image-captioning-base'\n",
    "    PROCESSOR_CAP = BlipProcessor.from_pretrained(MODEL_CAP_NAME)\n",
    "    MODEL_CAP = BlipForConditionalGeneration.from_pretrained(MODEL_CAP_NAME)\n",
    "\n",
    "    MODEL_DET_NAME = 'yolov8n.pt'\n",
    "    MODEL_DET = YOLO(MODEL_DET_NAME)\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image with AI caption and YOLO detection')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        # input text for conditional image captioning\n",
    "        text = st.text_input(\n",
    "            'Input text for conditional image captioning (if needed)', \n",
    "            ''\n",
    "        )\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # image caption model for uploaded image\n",
    "            caption = img_caption(\n",
    "                model=MODEL_CAP, \n",
    "                processor=PROCESSOR_CAP, \n",
    "                img=img, \n",
    "                text=text\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption=caption)\n",
    "            \n",
    "            # object detection model for uploaded image\n",
    "            objs, img_det = img_detect(\n",
    "                model=MODEL_DET, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img_det, caption='object detection', width=800)\n",
    "            st.divider()\n",
    "            st.caption('Objects dictionary:')\n",
    "            st.write(objs)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9692f84-e389-44e2-b325-be19fd9cb045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of AI model for image captioning\n",
    "# and object detection at the image\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def zeroshot(classifier, classes, img):\n",
    "    scores = classifier(\n",
    "        img,\n",
    "        candidate_labels=classes\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_ZERO_NAME = 'openai/clip-vit-base-patch16'\n",
    "    CLASSIFIER_ZERO = pipeline('zero-shot-image-classification', model=MODEL_ZERO_NAME)\n",
    "    CLASSES = [\n",
    "        'a photo of nature',\n",
    "        'a photo of cat',\n",
    "        'a photo of a party',\n",
    "        'a photo of a food'\n",
    "    ]\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and classifying it with zero-shot')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # classifying image with zero-shot modele\n",
    "            scores = zeroshot(\n",
    "                classifier=CLASSIFIER_ZERO, \n",
    "                classes=CLASSES, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption='zero-shot classification')\n",
    "            \n",
    "            # plot a diagram ith scores and scores output\n",
    "            st.divider()\n",
    "            df = pd.DataFrame(scores)\n",
    "            df = df.set_index('label')\n",
    "            st.bar_chart(df)\n",
    "            st.divider()\n",
    "            st.caption('Scores dictionary:')\n",
    "            st.write(scores)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5a314-6294-4005-8957-b8de29984ece",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.3. Add some OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78889b0e-e7a7-47a2-bc7b-450217f431f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of OCR model for text extracting \n",
    "# from image or PDF file\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_bytes\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pdf2img(pdf_bytes):\n",
    "    \"\"\"\n",
    "    Turns pdf file to set of jpeg images.\n",
    "\n",
    "    \"\"\"\n",
    "    images = convert_from_bytes(pdf_bytes.read())\n",
    "    return images\n",
    "\n",
    "\n",
    "def ocr_text(img, lang='eng'):\n",
    "    \"\"\"\n",
    "    Takes the text from image.\n",
    "    \n",
    "    :lang: language is `eng` by default,\n",
    "           use `eng+rus` for two languages in document\n",
    "\n",
    "    \"\"\"\n",
    "    text = str(pytesseract.image_to_string(\n",
    "        img,\n",
    "        lang=lang\n",
    "    ))\n",
    "    return text\n",
    "\n",
    "\n",
    "def ocr_text_dir(img_dir, lang='eng'):\n",
    "    \"\"\"\n",
    "    Takes the text from images in a folder.\n",
    "\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for img_name in tqdm(sorted(os.listdir(img_dir))):\n",
    "        if '.jpg' in img_name:\n",
    "            img = Image.open(f'{IMG_PATH}/{img_name}')\n",
    "            text_tmp = ocr_text(img, lang=lang)\n",
    "            text = ' '.join([text, text_tmp])\n",
    "    return text\n",
    "\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and extracting text from it')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you file or image')\n",
    "uploaded_file = st.file_uploader('Select a file (JPEG or PDF)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    lang = st.selectbox(\n",
    "            'Select language to extract ',\n",
    "            ('eng', 'rus', 'eng+rus')\n",
    "        )\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # image caption model for uploaded image\n",
    "            text = ocr_text(img, lang=lang)\n",
    "            st.divider()\n",
    "            st.write('#### Text extracted')\n",
    "            st.write(text)\n",
    "    elif '.pdf' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            imgs = pdf2img(uploaded_file)\n",
    "            text = ''\n",
    "            for img in imgs:\n",
    "                text_tmp = ocr_text(img, lang=lang)\n",
    "                text = ' '.join([text, text_tmp])\n",
    "            st.divider()\n",
    "            st.write('#### Text extracted')\n",
    "            st.write(text)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca128d06-5fe3-48ab-a6b7-6b6bea0ac856",
   "metadata": {},
   "source": [
    "### 3.4. Add some faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900be527-4853-46a3-86e3-13c96e64ec7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of DeepFace framework\n",
    "# for faces detection and recognotion\n",
    "\n",
    "import io\n",
    "import cv2\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from deepface import DeepFace\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def zeroshot(classifier, classes, img):\n",
    "    scores = classifier(\n",
    "        img,\n",
    "        candidate_labels=classes\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_ZERO_NAME = 'openai/clip-vit-base-patch16'\n",
    "    CLASSIFIER_ZERO = pipeline('zero-shot-image-classification', model=MODEL_ZERO_NAME)\n",
    "    CLASSES = [\n",
    "        'a photo of nature',\n",
    "        'a photo of cat',\n",
    "        'a photo of a party',\n",
    "        'a photo of a food'\n",
    "    ]\n",
    "    DEEPFACE_MODELS = [\n",
    "        'VGG-Face',\n",
    "        'Facenet',\n",
    "        'Facenet512',\n",
    "        'OpenFace',\n",
    "        'DeepFace',\n",
    "        'DeepID',\n",
    "        'ArcFace',\n",
    "        'Dlib',\n",
    "        'SFace',\n",
    "        'GhostFaceNet'\n",
    "    ]\n",
    "    DB_PATH = '/home/jovyan/dlba/topic_09/app/data/db'\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and classifying it with zero-shot and face recognition')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # classifying image with zero-shot modele\n",
    "            scores = zeroshot(\n",
    "                classifier=CLASSIFIER_ZERO, \n",
    "                classes=CLASSES, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption='zero-shot classification')\n",
    "            \n",
    "            # plot a diagram ith scores and scores output\n",
    "            st.divider()\n",
    "            df = pd.DataFrame(scores)\n",
    "            df = df.set_index('label')\n",
    "            st.bar_chart(df)\n",
    "            st.divider()\n",
    "            st.caption('Scores dictionary:')\n",
    "            st.write(scores)\n",
    "            \n",
    "            # faces detection and recognition\n",
    "            results = DeepFace.find(\n",
    "                img_path=np.array(img),  # face to find\n",
    "                db_path=f'{DB_PATH}',  # path to directory with faces\n",
    "                model_name=DEEPFACE_MODELS[0],\n",
    "                enforce_detection=False\n",
    "            )\n",
    "            st.divider()\n",
    "            st.caption('Faces recognition:')\n",
    "            found = []\n",
    "            for result in results:\n",
    "                name = result.identity.values\n",
    "                if name:\n",
    "                    found.append(\n",
    "                        name[0].replace(f'{DB_PATH}/', '').replace('.jpg', '')\n",
    "                    )\n",
    "            if found:\n",
    "                st.write(f'Found: {\" ,\".join(found)}')\n",
    "            else:\n",
    "                st.write('No known faces found')\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45be59-3147-4116-b8f0-9ab4555ea8a7",
   "metadata": {},
   "source": [
    "## 4. Move to developing app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd486b-d1a3-4f75-a181-372e0ee2e9b7",
   "metadata": {},
   "source": [
    "Let's get out Jupyter notebooks to a hardcore development process..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119322e9-8ec7-46c3-9352-1a803f1eadb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <font color='red'>HOME ASSIGNMENT (Final project)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81d902-323d-4dc5-997e-420d234b2e04",
   "metadata": {},
   "source": [
    "Your final project tasks are:\n",
    "1. Enlarge number of categories for uploaded images classification\n",
    "2. Change YOLO detection to segmentation model (use new version of YOLO, version 11 is released)\n",
    "3. Apply emotion detector model and add emotions for your friends faces recognition pipeline\n",
    "4. Implement application's page for OCR with Tesseract library\n",
    "5. Add text summarization option for text extracted in OCR page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495287f6-5d38-4ecf-9c81-2afe33fa639b",
   "metadata": {},
   "source": [
    "<font color='red'>Use application code snippets for the Final project, not the notebooks!</font> To run application use the following command from the terminal:\n",
    "\n",
    "`streamlit run Main_page.py --server.port 20000 --browser.gatherUsageStats False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9aaec1-3e7f-4590-983b-04c2aa5245f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
