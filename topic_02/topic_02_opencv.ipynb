{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b3aba7-5035-4488-9d7d-355aa9559976",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea36602-3e65-4b86-9142-6e8db4ccfd05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOPIC 2: Introduction to Computer Vision. Image processing with OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d2ca4-6cac-4bdd-a539-5edca3eeb9d2",
   "metadata": {},
   "source": [
    "### 1. Library installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ff88a-b1ff-4569-bda4-471153c8191c",
   "metadata": {},
   "source": [
    "Documentation for use of OpenCV with Python API [see here](https://docs.opencv.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df35b83-6ff6-4710-9103-979ebc5bb5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449d0aa-1ad0-4f82-a082-778cefc58557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25cd80-f039-40c0-b6a3-ff8590dae70c",
   "metadata": {},
   "source": [
    "### 2. Video processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e38a9f-a5fd-4558-a7fd-298fabd07f36",
   "metadata": {
    "tags": []
   },
   "source": [
    "Task came from work in [Russian Football Union](https://rfs.ru/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520854b-ba99-4807-b72b-0358ffba802d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -la /home/jovyan/__DATA/DLBA_F24/topic_02/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210fa9c-e19a-481e-8397-7b7cd80848ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vid_path = '/home/jovyan/__DATA/DLBA_F24/topic_02/videoplayback.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa2ab4-ab2e-4e36-a385-e754a605822f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open the video from the file\n",
    "\n",
    "cap = cv2.VideoCapture(vid_path)\n",
    "frames_cnt = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print('video has {} frames and rate {} fps (frames-per-second)'.format(\n",
    "    frames_cnt,\n",
    "    fps\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ad02e-805c-4d78-8cbe-933414a058a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_frames(vid_path, start_time, num_frames, save_dir):\n",
    "    \"\"\"\n",
    "    Function takes the path to video\n",
    "    and saves few frames to the disk.\n",
    "\n",
    "    :vid_path: path to video file\n",
    "    :start_time: where to start capturing frames\n",
    "    :num_frames: ho many frames to save\n",
    "    :save_dir: path to save to\n",
    "\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    frames_cnt = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    start_pos = int(start_time * fps)\n",
    "    end_pos = int(start_pos + num_frames)\n",
    "    if end_pos <= frames_cnt:\n",
    "        for frame_num in range(start_pos, end_pos):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            res, frame = cap.read()\n",
    "            if res:\n",
    "                file_name = '{}/frame_{}.png'.format(save_dir, frame_num)\n",
    "                cv2.imwrite(file_name, frame)\n",
    "    else:\n",
    "        print('out of video lenght')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821f827-c122-45d7-b054-e1af34818fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p football"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f49826-8627-4921-9611-5237cc23a0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "START_FRAME = 10\n",
    "FRAMES_TO_PROC = 3\n",
    "imgs_dir = 'football'\n",
    "get_frames(\n",
    "    vid_path,\n",
    "    START_FRAME,\n",
    "    FRAMES_TO_PROC,\n",
    "    imgs_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc12c9-7f4d-4e02-813c-defa23cc437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $imgs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5889f3e-125e-4262-8cd6-2f3a50b1791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(imgs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145b3c7-18f5-492b-888c-f3f6a3469983",
   "metadata": {},
   "source": [
    "### 3. Image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e385ee7b-f4ca-4adb-8ce5-688399920817",
   "metadata": {},
   "source": [
    "#### 3.1. Open an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db0af5-4448-46c9-8b51-efba9a3a1a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = f'{imgs_dir}/{os.listdir(imgs_dir)[0]}'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47405f2c-445e-4161-8576-44f408c398be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(file_path)\n",
    "assert img is not None, 'file could not be read, check if file exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477ce79-00bb-419b-bbd4-9da84035f931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84219fd3-6a4c-45e3-8f31-83a4c25be47f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc38b0-8664-45bf-8153-2a8f5cf2a0ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1ba12-c00f-449d-8aea-946ddd440ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's see our image\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e4d5f-ad32-4498-95f9-0c0fe931494a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RGBimage = cv2.cvtColor(BGRimage, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e543bd-a9cf-499e-a24c-99f92ee6c81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# access to one pixel\n",
    "px = img[100, 100]\n",
    "print(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ee131-35aa-4521-8260-37998054c832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accessing only blue pixel\n",
    "blue = img[100, 100, 0]\n",
    "print(blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171f184-647f-4c58-9fa5-649d3a57c191",
   "metadata": {},
   "source": [
    "#### 3.2. Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f1e36-cabf-4011-b945-6767d29a6b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw a box around player\n",
    "\n",
    "cv2.rectangle(\n",
    "    img,\n",
    "    (60, 800),\n",
    "    (180, 990),\n",
    "    (0, 255, 0),\n",
    "    2\n",
    ")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208cadb-7a31-44a9-9248-62dc92f9f4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put some text on the image\n",
    "\n",
    "cv2.putText(\n",
    "    img,\n",
    "    'player',\n",
    "    (60, 750),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    2,\n",
    "    (0, 255, 0),\n",
    "    3\n",
    ")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0c2ff-9588-4eb7-91ed-7e3518115f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get certain regions of images\n",
    "\n",
    "player = img[800:990, 60:180]\n",
    "plt.imshow(player)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910e4a8-d53e-43ca-a3f7-e146f7bc7d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image resize\n",
    "\n",
    "height, width = player.shape[:2]\n",
    "result = cv2.resize(\n",
    "    player,\n",
    "    (2 * width, 2 * height),\n",
    "    interpolation=cv2.INTER_CUBIC\n",
    ")\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04814bd-8f19-456d-af39-aad0f618812d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# affine transformation\n",
    "# all parallel lines in the original image\n",
    "# will still be parallel in the output image\n",
    "# we need three points from the input image\n",
    "# and their corresponding locations in the output image\n",
    "\n",
    "rows, cols, ch = player.shape\n",
    "pts1 = np.float32([\n",
    "    [25, 25],\n",
    "    [75, 25],\n",
    "    [25, 50]\n",
    "])\n",
    "pts2 = np.float32([\n",
    "    [10, 40],\n",
    "    [50, 50],\n",
    "    [15, 75]\n",
    "])\n",
    "\n",
    "M = cv2.getAffineTransform(pts1, pts2)\n",
    "dst = cv2.warpAffine(player, M, (cols, rows))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(player)\n",
    "plt.title('Input')\n",
    "plt.subplot(122)\n",
    "plt.imshow(dst)\n",
    "plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38871ea0-251c-4ab4-b36f-0357aa832242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# color histogram for the image\n",
    "\n",
    "color = ('b', 'g', 'r')\n",
    "for i, col in enumerate(color):\n",
    "    histr = cv2.calcHist(\n",
    "        [player],\n",
    "        [i],\n",
    "        None,\n",
    "        [256],\n",
    "        [0, 256]\n",
    "    )\n",
    "    plt.plot(histr, color=col)\n",
    "    plt.xlim([0, 256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b9be8-7838-4a76-b78b-791d809827b2",
   "metadata": {},
   "source": [
    "#### 3.3. Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea77447-b6a1-4a48-bbdf-3fdd6ee9f8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Canny Edge detection\n",
    "\n",
    "edges = cv2.Canny(player, 100, 200)\n",
    "plt.imshow(edges)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0d8d3-0fdb-4f40-91d8-7c7742063d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image Segmentation\n",
    "\n",
    "gray = cv2.cvtColor(player, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(\n",
    "    gray,\n",
    "    0,\n",
    "    255,\n",
    "    cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU\n",
    ")\n",
    "plt.imshow(thresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32167d-fad4-4a87-a614-47dcbc025c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interactive foreground extraction using GrabCut algorithm\n",
    "# https://docs.opencv.org/4.x/d8/d83/tutorial_py_grabcut.html\n",
    "\n",
    "mask = np.zeros(player.shape[:2], np.uint8)\n",
    "\n",
    "bgdModel = np.zeros((1, 65), np.float64)\n",
    "fgdModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "rect = (20, 15, 90, 160)\n",
    "cv2.grabCut(player, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "player_fg = player * mask2[:, :, np.newaxis]\n",
    "\n",
    "plt.imshow(player_fg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8bfeb-5571-4a7a-924f-badf0fade7d7",
   "metadata": {},
   "source": [
    "#### 3.4. Finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02118c74-63ae-418a-a3eb-d24d1692bcad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Template matching\n",
    "# https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html\n",
    "\n",
    "img_gr = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img2 = img_gr.copy()\n",
    "\n",
    "# our template will be the player\n",
    "# take it as part of the image\n",
    "template = cv2.cvtColor(player, cv2.COLOR_BGR2GRAY)\n",
    "w, h = template.shape[::-1]\n",
    "\n",
    "# All the 6 methods for comparison in a list\n",
    "methods = [\n",
    "    'TM_CCOEFF',\n",
    "    'TM_CCOEFF_NORMED',\n",
    "    'TM_CCORR',\n",
    "    'TM_CCORR_NORMED',\n",
    "    'TM_SQDIFF',\n",
    "    'TM_SQDIFF_NORMED'\n",
    "]\n",
    "\n",
    "for meth in methods:\n",
    "    img_ = img2.copy()\n",
    "    method = getattr(cv2, meth)\n",
    "\n",
    "    # Apply template Matching\n",
    "    res = cv2.matchTemplate(img_, template, method)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "    cv2.rectangle(img_, top_left, bottom_right, 255, 8)\n",
    "\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(res, cmap='gray')\n",
    "    plt.title('Matching Result')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img_, cmap='gray')\n",
    "    plt.title('Detected Point')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.suptitle(meth)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a88d9-9978-45ac-b1ac-1fd6df1b6ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Template matching with multiple objects\n",
    "\n",
    "img_rgb = img.copy()\n",
    "res = cv2.matchTemplate(img_, template, cv2.TM_CCOEFF_NORMED)\n",
    "# try to play with the threshold e.g. set it to .3\n",
    "threshold = .5\n",
    "loc = np.where(res >= threshold)\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(\n",
    "        img_rgb,\n",
    "        pt,\n",
    "        (pt[0] + w, pt[1] + h),\n",
    "        (255, 0, 0),\n",
    "        2\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(img_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b95704-3999-4ad9-a0fe-20a15b44ccf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
