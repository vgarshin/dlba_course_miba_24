{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a9dc66-1b05-4c81-97a2-54b84e9cb3e7",
   "metadata": {},
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839cb1a-b213-422c-a906-aac7b1d17e21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOPIC 3: Computer Vision advanced. Finetuning for image classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5743a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "\n",
    "# check if GPU available\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2ba7b-e02c-458c-8bff-962151543bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for training\n",
    "# you may try to tune them\n",
    "# for the HOME ASSIGNMENT part\n",
    "\n",
    "# batch size depends on resources\n",
    "# GPU or RAM memory\n",
    "BATCH_SIZE = 4\n",
    "# learning rate should be smaller\n",
    "# than the training LR for backbone CNN\n",
    "LR = .001\n",
    "# number of epochs to train\n",
    "N_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d864ce",
   "metadata": {},
   "source": [
    "#### 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b1e76-0c2b-478c-90c8-7906818e1b73",
   "metadata": {},
   "source": [
    "We are going to use part of [Food-101 dataset](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/). Wiil take two classes omly to finetune our classification CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b007947-d1d3-43e9-92c8-292f1dbd7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jovyan/__DATA/DLBA_F24/topic_03/food-101/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0a611-2ecd-4128-b91f-3053237bff81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -la $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285c154-1155-4075-8d01-30a1c17f66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $DATA_PATH/train/donuts | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5441e-60d4-4967-a245-b1409717e8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls $DATA_PATH/val/donuts | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89caeb89-c011-44ea-90c5-3076bed4d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $DATA_PATH/train/macarons | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b464e1c-fb39-4520-b1c3-254f0d9f9f8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ee6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# just normalization for validation data\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\n",
    "    # HOME ASSIGNMENT part\n",
    "    # you will have to make more data augmentations\n",
    "    # with help of Pytorch https://pytorch.org/vision/stable/transforms.html\n",
    "    # and make an experiment to answer the question\n",
    "    # how data augmentation affects training process\n",
    "    'train': v2.Compose([\n",
    "        v2.RandomResizedCrop(224),\n",
    "        # basic transformations\n",
    "        #v2.RandomHorizontalFlip(),\n",
    "        #v2.RandomVerticalFlip(),\n",
    "        #v2.RandomRotation([-15, 15]),\n",
    "        # color transformations use with caution\n",
    "        # and play with propabilities to apply\n",
    "        #v2.RandomChoice(\n",
    "        #    [\n",
    "        #        v2.ColorJitter(brightness=.5, hue=.3),\n",
    "        #        v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
    "        #        v2.ElasticTransform(alpha=250.0)\n",
    "        #    ],\n",
    "        #    p=[.1, .1, .1]\n",
    "        #),\n",
    "        # more about transformations you may read here\n",
    "        # https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(MEAN, STD)\n",
    "    ]),\n",
    "    'val': v2.Compose([\n",
    "        v2.Resize(256),\n",
    "        v2.CenterCrop(224),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(MEAN, STD)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2222706-d415-4af3-8f59-32b8f701f9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(DATA_PATH, x),\n",
    "        data_transforms[x]\n",
    "    )\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "class_names = image_datasets['train'].classes\n",
    "print('new classes:', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d145c8b",
   "metadata": {},
   "source": [
    "#### 4. Data sample visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b647575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"\n",
    "    Plot image for input tensor.\n",
    "\n",
    "    \"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean, std = np.array(MEAN), np.array(STD)\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "# get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "# make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f7759-57ae-4502-b621-95c5ec2f6fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of data samples (images) in dataloader\n",
    "len(dataloaders['train'].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb9e9e-ff5b-494d-a6a2-e9aa5860e6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of batches in dataloader\n",
    "len(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bb6b3",
   "metadata": {},
   "source": [
    "#### 5. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eafe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, \n",
    "                scheduler, device, n_epochs):\n",
    "    start_time = time.time()\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    best_epoch = 0\n",
    "    losses =[]\n",
    "    val_losses = []\n",
    "    accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # TRAIN PART\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['train']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # backpropagation part\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # collecting stats\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # tarining progress bar\n",
    "            if i % 10 == 0:\n",
    "                print(\n",
    "                    'Epoch {} - training [{}/{} ({:.0f}%)] loss: {:.3f}, accuracy: {:.2f}%'.format(\n",
    "                        epoch,\n",
    "                        i * len(inputs),\n",
    "                        len(dataloaders['train'].dataset),\n",
    "                        100 * i / len(dataloaders['train']),\n",
    "                        running_loss / (i + 1),\n",
    "                        float(running_corrects * 100) / float(BATCH_SIZE * (i + 1))\n",
    "                    ),\n",
    "                    end='\\r'\n",
    "                )\n",
    "\n",
    "        # epoch training stats\n",
    "        epoch_loss = running_loss / len(dataloaders['train'])\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders['train'].dataset)\n",
    "        losses.append(epoch_loss)\n",
    "        accs.append(epoch_acc)\n",
    "\n",
    "        # VALIDATION PART\n",
    "\n",
    "        model.eval()\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "\n",
    "                # no training here just predictions\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # collecting stats\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # epoch validation stats\n",
    "        epoch_val_loss = running_loss / len(dataloaders['val'])\n",
    "        epoch_val_acc = running_corrects.double() / len(dataloaders['val'].dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "\n",
    "        # run step for learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # saving best results\n",
    "        # NOTE: it is a good practice to use\n",
    "        # validation loss as an indicator\n",
    "        # where to stop training process\n",
    "        if epoch_val_acc > best_acc:\n",
    "            best_acc = epoch_val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "\n",
    "        print('Epoch {} - validation loss: {:.3f}, validation accuracy: {:.2f}%        '.format(\n",
    "            epoch,\n",
    "            epoch_val_loss,\n",
    "            epoch_val_acc * 100\n",
    "        ))\n",
    "\n",
    "    # final results\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best validation accuracy: {best_acc * 100:.2f}%, best epoch {best_epoch}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, losses, accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d26c6e",
   "metadata": {},
   "source": [
    "#### 6. Model to finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de9a71-e75d-4c5b-8197-b9073da94456",
   "metadata": {},
   "source": [
    "Let's take [ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) image classification pre-trained model as a backbone for our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58028d09-607c-4aa4-be83-3ff967fcbe21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819c31b-9c4f-4f9f-bdc4-67fb884dd8d6",
   "metadata": {},
   "source": [
    "##### 6.1. About the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7933ee-aeb0-4d96-8243-888df86a9092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cd05b-3b33-461b-a2af-78f159e7a908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9524110-2940-4159-8f56-82cb913c4f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0222f-a8ea-4779-ba64-0f9e1881a25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def json_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "\n",
    "imagenet_classes = json_data(\n",
    "    file_path=f'{DATA_PATH.replace(\"food-101/\", \"imagenet_class_index.json\")}'\n",
    ")\n",
    "print('all classes:', len(imagenet_classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639695da-37b1-4ef1-a39e-0e8308fa0d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imagenet_classes['100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99acba05-94b6-4c48-84c2-5a2d86236966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, v in imagenet_classes.items():\n",
    "    print(v[1], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a75239-e828-43e2-98e2-daf4fead9e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_model_prediction(model, img_path, device,\n",
    "                          classes=None, class_names=None):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    img = data_transforms['val'](img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        ax = plt.subplot(2, 2, 1)\n",
    "        ax.axis('off')\n",
    "        if classes:\n",
    "            title = classes[str(preds[0].numpy())]\n",
    "        if class_names:\n",
    "            title = class_names[preds[0]]\n",
    "        ax.set_title(title)\n",
    "        imshow(img.cpu().data[0])\n",
    "\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50439f47-4040-4052-8e04-38327b4d24c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_model_prediction(\n",
    "    model_ft,\n",
    "    img_path='/home/jovyan/__DATA/DLBA_F24/topic_03/test.jpg',\n",
    "    device=DEVICE,\n",
    "    classes=imagenet_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0533104a-7af5-4659-b4f1-019c916e262a",
   "metadata": {},
   "source": [
    "##### 6.2. Modify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 2 classes only, so we will replace last layer\n",
    "# with the new one with 20 outputs only\n",
    "# it can be generalized to `nn.Linear(num_ftrs, len(class_names))`\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# ...and put model to our device to work with\n",
    "model_ft = model_ft.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7190d8",
   "metadata": {},
   "source": [
    "#### 7. Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4de973-8eca-4643-9b81-2a2989e636f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our criterion for loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# set parameters for optimizer\n",
    "optimizer_ft = torch.optim.SGD(\n",
    "    model_ft.parameters(),\n",
    "    lr=LR,\n",
    "    momentum=.9\n",
    ")\n",
    "# or we can use another optimizer\n",
    "# you can do this experiment\n",
    "# for HOME ASSIGNMENT part\n",
    "#optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# decay learning rate (LR) by a factor of .1 every 5 epochs\n",
    "# you may also experiment with parameters of `step_size` and `decay`\n",
    "# for HOME ASSIGNMENT part\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer_ft,\n",
    "    step_size=5,\n",
    "    gamma=.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480b759-28b9-4e94-8e0b-e2c3ebd7f3cb",
   "metadata": {},
   "source": [
    "##### 7.1. Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed55dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft, losses, accs, val_losses, val_accs = train_model(\n",
    "    model_ft,\n",
    "    criterion,\n",
    "    optimizer_ft,\n",
    "    lr_scheduler,\n",
    "    device=DEVICE,\n",
    "    n_epochs=N_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26e5e7-15e8-43d3-b155-22fb477e7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('epochs')\n",
    "plt.title('Loss')\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.plot(val_losses, label='val loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('epochs')\n",
    "plt.title('Accuracy')\n",
    "plt.plot(accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='val accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae2a77-9f03-4614-bfcf-56f4a752d4e4",
   "metadata": {},
   "source": [
    "##### 7.2. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb3d9a-5fed-4294-80bc-7ca37b7ca0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_model_prediction(\n",
    "    model_ft,\n",
    "    img_path='/home/jovyan/__DATA/DLBA_F24/topic_03/food-101/macarons/1003207.jpg', \n",
    "    device=DEVICE,\n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783b1b3-71a2-4793-ba11-2e27deeeadab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_model_prediction(\n",
    "    model_ft,\n",
    "    img_path='/home/jovyan/__DATA/DLBA_F24/topic_03/food-101/donuts/1006079.jpg',\n",
    "    device=DEVICE,\n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffcd64-bed5-4ff9-a816-292cec848aed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <font color='red'>HOME ASSIGNMENT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700d5df-ac07-439d-8eb0-eb0c459ff974",
   "metadata": {},
   "source": [
    "There are many thing you can do with the finetuning process. Here are few of them:\n",
    "1. __(BASE)__ Try different data augmentation techniques and monitor the model finetuning performance. Does it get better? How has the speed of training changed? What about accuracy?\n",
    "2. __(ADVANCED)__ Make a few trials with different hyperparameters (learning rate, number of epochs, batch size) and observe the model's performance (speed and final accuracy).\n",
    "3. __(HARDCORE)__ Add one more class for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf08f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
