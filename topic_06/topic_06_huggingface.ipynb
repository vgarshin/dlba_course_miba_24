{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88975db6-ec8a-434a-ba05-91cc50999bac",
   "metadata": {},
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaf199-eb19-4820-a88a-b1de512d89eb",
   "metadata": {},
   "source": [
    "## TOPIC 6: Hugging Face Hub for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49ba0d-69aa-42bc-862e-e202416a87c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12239d2-9d70-4ca2-87ad-03911ce4857b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4d998-6e26-4fb0-a3b6-fce2dcf06fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you need to downgrade PyTorch for GPU usage\n",
    "# because our CUDA drivers for GPU are old\n",
    "# so uncomment lines below if you are in\n",
    "# the GPU environment\n",
    "\n",
    "#!pip uninstall -y torch torchvision\n",
    "#!pip install torch==2.0.1 torchvision==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb0daa-32ec-447c-8aea-a47595618431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check if GPU available\n",
    "# (works in GPU environment only)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device available:', DEVICE)\n",
    "\n",
    "# to get rid off warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'\n",
    "# env variable to set path to download models\n",
    "os.environ['HF_HOME'] = '/home/jovyan/dlba/topic_06/cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985dd1d9-0d32-46aa-aabc-7917c0db08b4",
   "metadata": {},
   "source": [
    "### 2. Disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5e22f-e1bf-42ab-a405-4e12228b2638",
   "metadata": {},
   "source": [
    "<font color='red'>__WARNING!!!__</font>\n",
    "\n",
    "Keep in mind free disk space for downloading models from Hub. Your local disk is 12 GB only, whereas modern architecture  models are large and can overfill your free space. Your server will stuck with disk overfilled and you will have to [contact support](https://t.me/simbaplatform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3476e2fc-b330-48cd-bbc0-f3e74a413bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!df -h | grep dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea47a64-70f9-4428-b4f6-eaf5b4666525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -ls ~/.cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994ab24-ac83-4596-aaff-6eb3f64370a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a place for Huggin Face Hub models\n",
    "\n",
    "!ls -ls ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cf40a-6733-452a-9613-20b8666f6aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use `rm -rf` !!! WITH CARE !!!\n",
    "\n",
    "!rm -rf ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21351eca-27a4-4264-a4cf-afda1161f791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a place for PyTorch models\n",
    "\n",
    "!ls -ls ~/.cache/torch/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5be959-28b9-4e09-93ea-aac4e9372a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/torch/hub/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9a03d-64cf-40c7-afc4-6bcbbfdeaa64",
   "metadata": {},
   "source": [
    "### 3. Models from the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e91cd7-9a90-4aa0-aa96-5441024b3234",
   "metadata": {},
   "source": [
    "#### 3.1. Warm up: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce60713-e4e4-499c-9385-578788214ab9",
   "metadata": {},
   "source": [
    "Start with [ResNet model](https://huggingface.co/microsoft/resnet-50) pre-trained on ImageNet-1k at resolution 224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfc23b-9f65-422f-ac11-69296a221a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ResNetForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a96bf-7b37-4785-9f61-7a426fa65cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open('imgs/burger.jpg')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa3e18-ac42-47ad-8276-f54d2b49cdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model and image processor\n",
    "# model will be downloaded automaticly\n",
    "# from Huggin Face Hub\n",
    "\n",
    "model_name = 'microsoft/resnet-50'\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = ResNetForImageClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd0bbb-8761-4468-bdef-326ff3a19c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert image to tensor\n",
    "inputs = processor(img, return_tensors='pt')\n",
    "\n",
    "# inference of the model\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6350b-3fed-4ea3-b874-14f0843bcfdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5978640-94bb-4ee6-a28c-6fc4764ca963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many classes?\n",
    "\n",
    "len(logits[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf2250-6992-4e1f-805e-82e8972176fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model predicts one of the 1000 ImageNet classes\n",
    "\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(\n",
    "    'class predicted:',\n",
    "    model.config.id2label[predicted_label]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e14766-be2d-4a92-a6d9-c1404e76e1c0",
   "metadata": {},
   "source": [
    "#### 3.2. More classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a1c00-6489-404c-9606-4bae2f9a0050",
   "metadata": {},
   "source": [
    "More interesting case of [Fine-Tuned Vision Transformer (ViT) for NSFW Image Classification](https://huggingface.co/Falconsai/nsfw_image_detection). The model can be used to detect NSFW (Not Safe for Work) content for the sites in the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526bd011-2603-4222-a841-7bfc3be070a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efee818-50fd-43aa-888c-bbf89cc6dfd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Police Academy rules!\n",
    "\n",
    "img = Image.open('imgs/blueoyster.jpg')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdbba8-bf85-4ac0-986e-ea5239ea38b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained('Falconsai/nsfw_image_detection')\n",
    "processor = ViTImageProcessor.from_pretrained('Falconsai/nsfw_image_detection')\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(\n",
    "    'class predicted:',\n",
    "    model.config.id2label[predicted_label]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b1db6-2338-45c2-b8e7-99f70cc80545",
   "metadata": {},
   "source": [
    "Where to find NSFW images? Think of it for yourself... Use the Internet if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da47a3-0d4f-4fc3-a786-62d7078b70d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# let's use `pipeline` for model inference\n",
    "pipe = pipeline(\n",
    "    'image-classification',\n",
    "    model='Falconsai/nsfw_image_detection'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae1873-6e3d-46dc-ba71-2191cec99b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# load the image into memory\n",
    "# you will need the URL for the image\n",
    "img_url = '<YOUR_URL_TO_IMAGE>'\n",
    "img = Image.open(\n",
    "    requests.get(img_url, stream=True).raw\n",
    ").convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53320d-ce2b-4580-9bac-7477902da8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# easy to use a pipeline to classify image\n",
    "\n",
    "results = pipe(img)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659168c-1ba3-427e-b53f-59929a486a36",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4. Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9005d9-30f7-4c5c-8621-bfc9414b78a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Work with an arbitary [detection model](https://huggingface.co/facebook/detr-resnet-50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b086d2e-5cf5-4023-a727-b3521c436ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ddf995-7bf7-4be0-80da-4c43bc813d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51a54c-91fc-4209-91c3-339fc786688d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jovyan/__DATA/DLBA_F24/topic_04'\n",
    "img_path = f'{DATA_PATH}/ace.jpg'\n",
    "img = Image.open(img_path)\n",
    "img_ = cv2.imread(img_path)\n",
    "img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a72f9-f930-415c-b4de-722a68a07715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model and image processor\n",
    "model_name = 'facebook/detr-resnet-50'\n",
    "processor = DetrImageProcessor.from_pretrained(model_name, revision='no_timm')\n",
    "model = DetrForObjectDetection.from_pretrained(model_name, revision='no_timm')\n",
    "\n",
    "# inference for detection\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a426b0-0a24-4646-8ac4-2ab23f54cde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "th = .75\n",
    "target_sizes = torch.tensor([img.size[::-1]])\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs,\n",
    "    target_sizes=target_sizes,\n",
    "    threshold=th\n",
    ")[0]\n",
    "\n",
    "# results and bbox drawing\n",
    "for score, label, box in zip(results['scores'], \n",
    "                             results['labels'], \n",
    "                             results['boxes']):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    lbl = model.config.id2label[label.item()]\n",
    "    print(\n",
    "            f'detected {lbl} with confidence',\n",
    "            f'{round(score.item(), 2)} at location {box}'\n",
    "    )\n",
    "\n",
    "    top_left = (int(box[0]), int(box[1]))\n",
    "    bottom_right = (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(img_, top_left, bottom_right, (0, 255, 0), 3)\n",
    "    cv2.putText(\n",
    "        img_,\n",
    "        lbl,\n",
    "        top_left,\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        2,\n",
    "        (0, 255, 0),\n",
    "        3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2589074-673d-4f09-86c8-c6a7d98dd09f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.imshow(img_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f383baf-7ce8-4cac-a499-f49b5ff4bbb6",
   "metadata": {},
   "source": [
    "__NOTE:__\n",
    "You may finetune this model like we did in previous classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133dc58-8d82-4c72-8f95-3fa584f45325",
   "metadata": {},
   "source": [
    "#### 3.3. Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983ddd7-a48b-4104-8b7b-d1c91e0f1eb3",
   "metadata": {},
   "source": [
    "Now let's [BLIP](https://huggingface.co/Salesforce/blip-image-captioning-base). \n",
    "\n",
    "BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation) model is used image captioning pretrained on COCO dataset - base architecture (with ViT base backbone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce04eeb-2cbb-4719-b75c-d1a4dd6ac8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22522b2-3f62-4a05-8e57-3ffe93f22223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'Salesforce/blip-image-captioning-base'\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bd711-a74b-4acb-89e4-1f19c2c37780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for free space left...\n",
    "!df -h | grep dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2164c-82a6-42ff-a792-50fd818ff3e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open('imgs/burger.jpg')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31471f3d-f9a6-4696-8079-b6f434d7f36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_caption(img):\n",
    "    # conditional image captioning\n",
    "    text = 'a photography of'\n",
    "    inputs = processor(img, text, return_tensors='pt')\n",
    "    out = model.generate(**inputs)\n",
    "    print(\n",
    "        'conditional image captioning:',\n",
    "        processor.decode(out[0], skip_special_tokens=True)\n",
    "    )\n",
    "\n",
    "    # unconditional image captioning\n",
    "    inputs = processor(img, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    print(\n",
    "        'unconditional image captioning',\n",
    "        processor.decode(out[0], skip_special_tokens=True)\n",
    "    )\n",
    "\n",
    "\n",
    "img_caption(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265eed6-b022-4e46-ae68-e5b09d56df2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open('imgs/blueoyster.jpg')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img_caption(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818d736-72fc-42c9-b203-c2d4f380bdf6",
   "metadata": {},
   "source": [
    "#### 3.4. More for image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c440dd5-71d3-4f92-954a-425e50cb442b",
   "metadata": {},
   "source": [
    "Now try [one more image captioning model](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning) form the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9cb7e-a8ac-495e-9cbc-c3624eca829c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e24d05-901b-40b5-8181-45e5011ba1fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'nlpconnect/vit-gpt2-image-captioning'\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faac95-91f0-4891-8bda-3bb8c5a19760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open('imgs/burger.jpg')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9888f-09ab-45f2-a53f-07127e901373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters to manage model's performance\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {'max_length': max_length, 'num_beams': num_beams}\n",
    "\n",
    "# run the model\n",
    "pixel_values = feature_extractor(images=img, return_tensors='pt').pixel_values\n",
    "output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "preds = [pred.strip() for pred in preds]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7e69e-0a06-45f9-9669-06c6349d1b67",
   "metadata": {},
   "source": [
    "...or with use of `pipeline` from `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7554df-e51b-4a53-8b2b-b78c0c6c8b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_img2txt = pipeline('image-to-text', model='nlpconnect/vit-gpt2-image-captioning')\n",
    "results = pipe_img2txt('imgs/blueoyster.jpg')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97bd4b-9669-49a8-8393-1aa84ec14e2a",
   "metadata": {},
   "source": [
    "#### 3.5. Text-to-Image model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66bb30-8eb1-4e7d-b4ef-d242245709bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Stable diffusion](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5) based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9106e-9f52-4a36-baed-150e78a66f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install diffusers\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ada9b-a154-4fae-88e5-0b58c0a0e732",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font color='red'>__WARNING!!!__</font>\n",
    "\n",
    "(1) Keep in mind free disk space for downloading models from Hub. Diffusion models ARE VERY LARGE.\n",
    "\n",
    "(2) You need GPU environment to run image generating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba399b33-feba-4332-93d6-0b4a529bc045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/huggingface/hub\n",
    "!rm -rf /home/jovyan/dlba/topic_06/cache/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd0231-148c-40fe-a3a0-cf4c50333db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!df -h | grep dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d034a-a8be-4a25-9047-a3f0cd6d0fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d7595-e2f7-4d42-901f-0c70c8bc65db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just take model from the Hub\n",
    "# and create a pipeline for work\n",
    "\n",
    "model_name = 'sd-legacy/stable-diffusion-v1-5'\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a7ef0-ec3e-46b3-8d59-82dc59e9d574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put model to GPU to run fast\n",
    "pipe = pipe.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd42c3-fdfb-4288-92bc-7a1199fade0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need a brief description of what we want to get\n",
    "prompt = 'a bald guy with earphones is giving an online lecture'\n",
    "\n",
    "# ...and here is the image\n",
    "img = pipe(prompt).images[0]\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b0952-50ae-43fd-852f-63c399691c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
