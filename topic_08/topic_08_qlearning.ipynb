{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7faee03b-9680-4aef-a5d5-fe34b0375c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84d921-8c25-43ac-ba73-48c83ee91a4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOPIC 8: More Tasks for Deep Learning. Reinforcement Learning (Q-learning algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733ad08-4435-40fd-b93d-5ff063f7e4c5",
   "metadata": {},
   "source": [
    "__GOAL:__  to teach a bot to reach its destination using the Q-Learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb2f39-e2c1-4cac-b3f8-8661d4b3cfe7",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a05281-5e3f-4b19-9592-cd152feac8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pylab as pl \n",
    "import networkx as nx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5ea13-8d5e-4493-9f44-1fd9f05a8b39",
   "metadata": {},
   "source": [
    "### 2. Define and plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550bfe5-f873-4532-a2f2-cb46bafc67a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edges = [\n",
    "    (0, 1), (1, 5), (5, 6), (5, 4), (1, 2),\n",
    "    (1, 3), (9, 10), (2, 4), (0, 6), (6, 7),\n",
    "    (8, 9), (7, 8), (1, 7), (3, 9),\n",
    "    #(3, 10)  # add one edge for fun\n",
    "]\n",
    "goal = 10\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw_networkx_nodes(G, pos)\n",
    "nx.draw_networkx_edges(G, pos)\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8471d1-cf0b-4d0c-a152-4591808b9308",
   "metadata": {},
   "source": [
    "### 3. Reward for the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a88af5-14f5-4e64-85ed-1c5d08a1d7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MATRIX_SIZE = 11\n",
    "M = np.matrix(np.ones(shape =(MATRIX_SIZE, MATRIX_SIZE))) \n",
    "M *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9f8c1-c3ec-4c01-aa65-888daf6a6cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('points:')\n",
    "for point in edges:\n",
    "    print(point)\n",
    "    if point[1] == goal:\n",
    "        M[point] = 100\n",
    "    else:\n",
    "        M[point] = 0\n",
    "\n",
    "    if point[0] == goal:\n",
    "        M[point[::-1]] = 100\n",
    "    else:\n",
    "        M[point[::-1]] = 0\n",
    "\n",
    "M[goal, goal] = 100\n",
    "print('\\nmatrix:\\n', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165fb8ae-82f4-4093-9dd3-e8e60a74076b",
   "metadata": {},
   "source": [
    "### 4. Utility functions for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ba249-3599-425c-a5c4-75b6d84ad2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q = np.matrix(np.zeros([MATRIX_SIZE, MATRIX_SIZE]))\n",
    "\n",
    "gamma = 0.75  # learning parameter\n",
    "initial_state = 1\n",
    "\n",
    "\n",
    "# determines the available actions for a given state\n",
    "def available_actions(state):\n",
    "    current_state_row = M[state, ]\n",
    "    available_action = np.where(current_state_row >= 0)[1]\n",
    "    return available_action\n",
    "\n",
    "\n",
    "available_action = available_actions(initial_state)\n",
    "\n",
    "\n",
    "# chooses one of the available actions at random \n",
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_action, 1))\n",
    "    return next_action\n",
    "\n",
    "\n",
    "action = sample_next_action(available_action)\n",
    "\n",
    "\n",
    "# updates the Q-Matrix according to the path chosen\n",
    "def update(current_state, action, gamma):\n",
    "    max_index = np.where(Q[action, ] == np.max(Q[action, ]))[1]\n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, size=1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    max_value = Q[action, max_index]\n",
    "    Q[current_state, action] = M[current_state, action] + gamma * max_value\n",
    "    if (np.max(Q) > 0):\n",
    "        return (np.sum(Q / np.max(Q) * 100))\n",
    "    else:\n",
    "        return (0)\n",
    "\n",
    "\n",
    "update(initial_state, action, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50171319-2df0-43b6-b191-ae11090bd322",
   "metadata": {},
   "source": [
    "### 5. Training and evaluating the bot using the Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1e96f-a1eb-4801-8f75-276e6ab94bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(1000):\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "    available_action = available_actions(current_state)\n",
    "    action = sample_next_action(available_action)\n",
    "    score = update(current_state, action, gamma)\n",
    "    scores.append(score)\n",
    "\n",
    "print('trained Q matrix:\\n')\n",
    "print(np.round(Q / np.max(Q) * 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee0856-0b91-4b68-8bd6-1b8e5cbe59df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "current_state = 0\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != 10:\n",
    "    next_step_index = np.where(Q[current_state, ] == np.max(Q[current_state, ]))[1]\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size = 1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "\n",
    "print('most efficient path:')\n",
    "print(steps)\n",
    "\n",
    "pl.plot(scores)\n",
    "pl.xlabel('No of iterations')\n",
    "pl.ylabel('Reward gained')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9162a6-fd73-4049-a818-3e642c2228bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
